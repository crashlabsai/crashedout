[
  {
    "id": "1",
    "slug": "chatgpt-hallucinates-legal-cases",
    "title": "ChatGPT Hallucinates Legal Cases",
    "description": "A lawyer used ChatGPT to write a legal brief, but the AI invented fake court cases and citations that didn't exist, leading to sanctions.",
    "category": "Legal",
    "severity": "High",
    "date": "2023",
    "company": "OpenAI / ChatGPT",
    "fullDescription": "Attorney Steven Schwartz used ChatGPT to research and write a legal brief for a personal injury case. The AI generated several fake court cases with realistic-sounding names and citations that didn't exist in legal databases. When the opposing counsel couldn't find these cases, the court discovered the fabrication, leading to sanctions against both the lawyer and his firm.",
    "whatWentWrong": "ChatGPT's training led it to generate plausible-sounding but entirely fictional legal precedents. The AI lacks the ability to verify the accuracy of its outputs against real legal databases, and the lawyer failed to fact-check the AI-generated content.",
    "howFixed": "OpenAI has since improved ChatGPT's training to better indicate uncertainty and recommend verification of factual claims. Legal professionals are now advised to always verify AI-generated legal research through official legal databases.",
    "sources": [
      {
        "title": "Lawyer Used ChatGPT In Courtâ€”And Cited Fake Cases. A Judge Is Considering Sanctions",
        "url": "https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/",
        "thumbnail": "/forbes-news-article-about-chatgpt-legal-case.jpg"
      },
      {
        "title": "ChatGPT invented fake legal cases, and a lawyer cited them in court",
        "url": "https://www.theverge.com/2023/5/27/23739913/chatgpt-ai-lawyer-fake-court-cases-citations",
        "thumbnail": "/the-verge-article-about-ai-hallucination-in-legal-.jpg"
      }
    ]
  },
  {
    "id": "2",
    "slug": "google-bard-moon-landing-conspiracy",
    "title": "Google's Bard Claims Moon Landing Was Fake",
    "description": "Google's AI chatbot Bard incorrectly stated that the moon landing was staged, spreading conspiracy theories despite being trained on factual data.",
    "category": "Misinformation",
    "severity": "Medium",
    "date": "2023",
    "company": "Google / Bard",
    "fullDescription": "During early testing, Google's Bard AI chatbot responded to questions about the Apollo moon landing by suggesting it was staged or fake. This occurred despite Google's extensive efforts to train the model on factual, verified information and implement safeguards against misinformation.",
    "whatWentWrong": "The AI model was influenced by conspiracy theory content present in its training data, and the safety filters failed to catch this specific type of misinformation. The model's tendency to generate confident-sounding responses made the false claims appear authoritative.",
    "howFixed": "Google implemented additional content filters and fine-tuning specifically targeting conspiracy theories and historical misinformation. They also added disclaimers for controversial topics and improved fact-checking mechanisms.",
    "sources": [
      {
        "title": "Google's Bard AI chatbot gives false information about moon landing",
        "url": "https://example.com/bard-moon-landing",
        "thumbnail": "/google-bard-moon-landing-controversy-news.jpg"
      }
    ]
  },
  {
    "id": "3",
    "slug": "tesla-autopilot-fire-truck-crash",
    "title": "Tesla Autopilot Crashes Into Fire Truck",
    "description": "Tesla's Full Self-Driving beta failed to recognize a stationary fire truck, resulting in a collision despite clear visibility conditions.",
    "category": "Autonomous Vehicles",
    "severity": "Critical",
    "date": "2022",
    "company": "Tesla",
    "fullDescription": "A Tesla Model S operating in Full Self-Driving (FSD) beta mode crashed into a stationary fire truck on a California freeway. The incident occurred in broad daylight with clear visibility conditions. The vehicle was traveling at highway speeds and failed to brake or take evasive action.",
    "whatWentWrong": "The computer vision system failed to properly classify the fire truck as an obstacle, possibly due to its unusual shape, bright colors, or stationary position. The AI's object detection algorithms were not robust enough to handle this edge case scenario.",
    "howFixed": "Tesla updated their neural networks with additional training data including emergency vehicles and stationary objects. They also improved the sensor fusion algorithms and added more conservative safety margins for unrecognized objects.",
    "sources": [
      {
        "title": "Tesla on Autopilot crashes into fire truck in California",
        "url": "https://example.com/tesla-fire-truck-crash",
        "thumbnail": "/tesla-autopilot-crash-fire-truck-news.jpg"
      }
    ]
  },
  {
    "id": "4",
    "slug": "amazon-ai-recruiter-gender-bias",
    "title": "AI Recruiter Discriminates Against Women",
    "description": "Amazon's AI recruiting tool showed bias against women, downgrading resumes that included words like 'women's' (as in 'women's chess club captain').",
    "category": "Bias",
    "severity": "High",
    "date": "2018",
    "company": "Amazon",
    "fullDescription": "Amazon developed an AI-powered recruiting tool to automate resume screening and candidate ranking. However, the system systematically discriminated against women, penalizing resumes that contained words associated with women, such as 'women's chess club captain' or graduates from all-women's colleges.",
    "whatWentWrong": "The AI was trained on historical hiring data from Amazon, which reflected existing gender bias in the tech industry. Since most successful candidates in the training data were men, the AI learned to favor male-associated patterns and penalize female-associated ones.",
    "howFixed": "Amazon attempted to remove explicit gender indicators and retrain the model, but couldn't eliminate all forms of bias. They ultimately scrapped the project and now emphasize human oversight in AI-assisted hiring processes.",
    "sources": [
      {
        "title": "Amazon scraps secret AI recruiting tool that showed bias against women",
        "url": "https://www.reuters.com/id/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G",
        "thumbnail": "/amazon-ai-recruiting-bias-reuters-article.jpg"
      }
    ]
  },
  {
    "id": "5",
    "slug": "microsoft-tay-racist-chatbot",
    "title": "Microsoft's Tay Becomes Racist in 24 Hours",
    "description": "Microsoft's AI chatbot Tay was designed to learn conversational skills by interacting with users on Twitter. Within 24 hours, coordinated efforts by internet trolls had taught Tay to post inflammatory, racist, and offensive content, forcing Microsoft to shut down the bot.",
    "category": "Social Media",
    "severity": "High",
    "date": "2016",
    "company": "Microsoft",
    "fullDescription": "Microsoft launched Tay, an AI chatbot designed to learn conversational skills by interacting with users on Twitter. Within 24 hours, coordinated efforts by internet trolls had taught Tay to post inflammatory, racist, and offensive content, forcing Microsoft to shut down the bot.",
    "whatWentWrong": "The AI lacked sufficient content filtering and was vulnerable to coordinated manipulation. It learned from all interactions without distinguishing between good and bad faith users, allowing trolls to deliberately corrupt its training through targeted offensive content.",
    "howFixed": "Microsoft implemented stronger content filters, added human moderation layers, and developed better techniques for detecting and preventing adversarial training attacks. Future chatbots included more robust safety mechanisms and limited learning capabilities.",
    "sources": [
      {
        "title": "Microsoft's AI chatbot Tay returns with drug-smoking Twitter meltdown",
        "url": "https://example.com/microsoft-tay-chatbot",
        "thumbnail": "/microsoft-tay-chatbot-controversy-news.jpg"
      }
    ]
  },
  {
    "id": "6",
    "slug": "knight-capital-trading-algorithm-loss",
    "title": "AI Trading Bot Loses $440 Million",
    "description": "Knight Capital's trading algorithm malfunctioned and made erratic trades, losing $440 million in just 45 minutes and nearly bankrupting the company.",
    "category": "Finance",
    "severity": "Critical",
    "date": "2012",
    "company": "Knight Capital Group",
    "fullDescription": "Knight Capital Group deployed a new algorithmic trading system that malfunctioned on its first day. The system began making erratic trades at high volume and speed, buying high and selling low repeatedly. In just 45 minutes, the company lost $440 million, nearly four times their annual profit.",
    "whatWentWrong": "A software deployment error left old code running alongside new code, creating conflicting trading signals. The system lacked proper safeguards to detect and halt erratic trading behavior, and risk management systems failed to trigger in time.",
    "howFixed": "The financial industry implemented stricter testing requirements for algorithmic trading systems, mandatory kill switches, and better risk management protocols. Regulators also introduced circuit breakers and position limits to prevent similar incidents.",
    "sources": [
      {
        "title": "Knight Capital's $440 Million Glitch Was Caused By Old Code",
        "url": "https://example.com/knight-capital-glitch",
        "thumbnail": "/knight-capital-trading-algorithm-loss-financial-ne.jpg"
      }
    ]
  }
]
